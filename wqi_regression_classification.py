# -*- coding: utf-8 -*-
"""WQI_Regression_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A1wjNssAMo9S91miENaMWL8WcmhwY1ZX
"""

import pandas as pd

# Load the dataset
data = pd.read_csv("/content/wqi_data_with_wqc (1).csv")

# Drop WQC as it is not needed for regression
data = data.drop(columns=['WQC'])

# Separate features (X) and target (y)
X = data.drop(columns=['WQI'])
y = data['WQI']

from sklearn.preprocessing import StandardScaler, MinMaxScaler

# Initialize scalers
min_max_scaler = MinMaxScaler()
standard_scaler = StandardScaler()

# Normalize the data
X_normalized = min_max_scaler.fit_transform(X)

# Standardize the data
X_standardized = standard_scaler.fit_transform(X_normalized)

from sklearn.model_selection import train_test_split

# Split the data: 80% train, 20% temporary split (to later split into 10% test, 10% evaluation)
X_train, X_temp, y_train, y_temp = train_test_split(X_standardized, y, test_size=0.2, random_state=42)

# Split the temporary set into test and evaluation sets (50% each of the 20%, so 10% of the total each)
X_test, X_eval, y_test, y_eval = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# Initialize the model
model = LinearRegression()

# Train the model
model.fit(X_train, y_train)

# Predict on the test set
y_pred = model.predict(X_test)

# Evaluate the model
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print("Mean Absolute Error (MAE):", mae)
print("Mean Squared Error (MSE):", mse)
print("Root Mean Squared Error (RMSE):", rmse)
print("R-squared (R2) Score:", r2)

import seaborn as sns
import matplotlib.pyplot as plt

# Visualize results with scatter plot
sns.set(style="whitegrid")
plt.figure(figsize=(8, 6))
sns.scatterplot(x=y_test, y=y_pred, marker='o', color='skyblue', edgecolor='black')
sns.lineplot(x=y_test, y=y_test, color='red', linestyle='--', label='Perfect Prediction')
plt.xlabel("Actual WQI", fontsize=14)
plt.ylabel("Predicted WQI", fontsize=14)
plt.title("Actual vs Predicted WQI (Linear Regression)", fontsize=16)
plt.legend(fontsize=12)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.tight_layout()
plt.savefig("regress.png")
plt.show()

import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv("/content/wqi_data_with_wqc (1).csv")

# Drop WQC as it is not needed for regression
data = data.drop(columns=['WQC'])

# Separate features (X) and target (y)
X = data.drop(columns=['WQI'])
y = data['WQI']

# Initialize scalers
min_max_scaler = MinMaxScaler()
standard_scaler = StandardScaler()

# Normalize the data
X_normalized = min_max_scaler.fit_transform(X)

# Standardize the data
X_standardized = standard_scaler.fit_transform(X_normalized)

# Split the data: 80% train, 20% temporary split (to later split into 10% test, 10% evaluation)
X_train, X_temp, y_train, y_temp = train_test_split(X_standardized, y, test_size=0.2, random_state=42)

# Split the temporary set into test and evaluation sets (50% each of the 20%, so 10% of the total each)
X_test, X_eval, y_test, y_eval = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# List of models to evaluate
models = {
    "Linear Regression": LinearRegression(),
    "Random Forest": RandomForestRegressor(random_state=42),
    "Support Vector Regression (SVR)": SVR(),
    "Decision Tree": DecisionTreeRegressor(random_state=42),
    "Multi-layer Perceptron (MLP)": MLPRegressor(random_state=42),
    "Gradient Boosting Regressor": GradientBoostingRegressor(random_state=42)
}

# Initialize dictionary to store results
results = {}

# Train, predict, and evaluate each model
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)

    results[name] = {
        "MAE": mae,
        "MSE": mse,
        "RMSE": rmse,
        "R2": r2
    }

    print(f"\n{name} Results:")
    print("Mean Absolute Error (MAE):", mae)
    print("Mean Squared Error (MSE):", mse)
    print("Root Mean Squared Error (RMSE):", rmse)
    print("R-squared (R2) Score:", r2)

    # Visualize results with scatter plot
    sns.set(style="whitegrid")
    plt.figure(figsize=(8, 6))
    sns.scatterplot(x=y_test, y=y_pred, marker='o', color='skyblue', edgecolor='black')
    sns.lineplot(x=y_test, y=y_test, color='red', linestyle='--', label='Perfect Prediction')
    plt.xlabel("Actual WQI", fontsize=14)
    plt.ylabel("Predicted WQI", fontsize=14)
    plt.title(f"Actual vs Predicted WQI ({name})", fontsize=16)
    plt.legend(fontsize=12)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    plt.tight_layout()
    plt.savefig(f"{name.replace(' ', '_').lower()}_regress.png")
    plt.show()



import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Load the dataset
data = pd.read_csv("/content/wqi_data_with_wqc (1).csv")

# Separate features (X) and target (Y)
X = data.drop(columns=['WQI', 'WQC'])
y = data['WQC']

# Encode the target labels to numeric values
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Initialize scalers
min_max_scaler = MinMaxScaler()
standard_scaler = StandardScaler()

# Normalize the data
X_normalized = min_max_scaler.fit_transform(X)

# Standardize the data
X_standardized = standard_scaler.fit_transform(X_normalized)

# Split the data: 80% train, 20% temporary split (to later split into 10% test, 10% evaluation)
X_train, X_temp, y_train, y_temp = train_test_split(X_standardized, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)

# Split the temporary set into test and evaluation sets (50% each of the 20%, so 10% of the total each)
X_test, X_eval, y_test, y_eval = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)

# List of models to evaluate
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000, random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42),
    "Support Vector Machine (SVM)": SVC(probability=True, random_state=42),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Multi-layer Perceptron (MLP)": MLPClassifier(random_state=42, max_iter=1000),
    "Gradient Boosting Classifier": GradientBoostingClassifier(random_state=42),
    "Naive Bayes": GaussianNB(),
    "K-Nearest Neighbors (KNN)": KNeighborsClassifier()
}

# Train, predict, and evaluate each model
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred, target_names=label_encoder.classes_)
    cm = confusion_matrix(y_test, y_pred)

    print(f"\n{name} Results:")
    print("Accuracy:", accuracy)
    print("Classification Report:\n", report)
    print("Confusion Matrix:\n", cm)

    # Visualize confusion matrix
    sns.set(style="whitegrid")
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
    plt.xlabel("Predicted", fontsize=14)
    plt.ylabel("Actual", fontsize=14)
    plt.title(f"Confusion Matrix ({name})", fontsize=16)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    plt.tight_layout()
    plt.savefig(f"{name.replace(' ', '_').lower()}_confusion_matrix.png")
    plt.show()

    # ROC Curve
    plt.figure(figsize=(6, 4))
    for i in range(len(label_encoder.classes_)):
        fpr, tpr, _ = roc_curve(y_test, y_prob[:, i], pos_label=i)
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, lw=2, label=f'Class {label_encoder.classes_[i]} (area = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate', fontsize=14)
    plt.ylabel('True Positive Rate', fontsize=14)
    plt.title(f'Receiver Operating Characteristic ({name})', fontsize=16)
    plt.legend(loc="lower right", fontsize=12)
    plt.tight_layout()
    plt.savefig(f"{name.replace(' ', '_').lower()}_roc_curve.png")
    plt.show()

    # Precision-Recall Curve
    plt.figure(figsize=(6, 4))
    for i in range(len(label_encoder.classes_)):
        precision, recall, _ = precision_recall_curve(y_test, y_prob[:, i], pos_label=i)
        plt.plot(recall, precision, lw=2, label=f'Class {label_encoder.classes_[i]}')
    plt.xlabel('Recall', fontsize=14)
    plt.ylabel('Precision', fontsize=14)
    plt.title(f'Precision-Recall Curve ({name})', fontsize=16)
    plt.legend(loc="lower left", fontsize=12)
    plt.tight_layout()
    plt.savefig(f"{name.replace(' ', '_').lower()}_precision_recall_curve.png")
    plt.show()

import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv("wqi_data_with_wqc (1).csv")

# Drop WQC as it is not needed for regression
data = data.drop(columns=['WQC'])

# Separate features (X) and target (y)
X = data.drop(columns=['WQI'])
y = data['WQI']

# Initialize scalers
min_max_scaler = MinMaxScaler()
standard_scaler = StandardScaler()

# Normalize the data
X_normalized = min_max_scaler.fit_transform(X)

# Standardize the data
X_standardized = standard_scaler.fit_transform(X_normalized)

# Split the data: 80% train, 20% temporary split (to later split into 10% test, 10% evaluation)
X_train, X_temp, y_train, y_temp = train_test_split(X_standardized, y, test_size=0.2, random_state=42)

# Split the temporary set into test and evaluation sets (50% each of the 20%, so 10% of the total each)
X_test, X_eval, y_test, y_eval = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# List of models to evaluate
models = {
    "Random Forest": RandomForestRegressor(random_state=42),
    "Support Vector Regression (SVR)": SVR(),
    "Decision Tree": DecisionTreeRegressor(random_state=42),
    "Multi-layer Perceptron (MLP)": MLPRegressor(random_state=42),
    "Gradient Boosting Regressor": GradientBoostingRegressor(random_state=42)
}

# Initialize dictionary to store results
results = {}

# Train, predict, and evaluate each model
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)

    results[name] = {
        "MAE": mae,
        "MSE": mse,
        "RMSE": rmse,
        "R2": r2
    }

    print(f"\n{name} Results:")
    print("Mean Absolute Error (MAE):", mae)
    print("Mean Squared Error (MSE):", mse)
    print("Root Mean Squared Error (RMSE):", rmse)
    print("R-squared (R2) Score:", r2)

    # Visualize results with scatter plot
    sns.set(style="whitegrid")
    plt.figure(figsize=(8, 6))
    sns.scatterplot(x=y_test, y=y_pred, marker='o', color='skyblue', edgecolor='black')
    sns.lineplot(x=y_test, y=y_test, color='red', linestyle='--', label='Perfect Prediction')
    plt.xlabel("Actual WQI", fontsize=14)
    plt.ylabel("Predicted WQI", fontsize=14)
    plt.title(f"Actual vs Predicted WQI ({name})", fontsize=16)
    plt.legend(fontsize=12)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    plt.tight_layout()
    plt.savefig(f"{name.replace(' ', '_').lower()}_regress.png")
    plt.show()

import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Load the dataset
data = pd.read_csv("/content/wqi_data_with_wqc (1).csv")

# Separate features (X) and target (Y)
X = data.drop(columns=['WQI', 'WQC'])
y = data['WQC']

# Encode the target labels to numeric values
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Initialize scalers
min_max_scaler = MinMaxScaler()
standard_scaler = StandardScaler()

# Normalize the data
X_normalized = min_max_scaler.fit_transform(X)

# Standardize the data
X_standardized = standard_scaler.fit_transform(X_normalized)

# Split the data: 80% train, 20% temporary split (to later split into 10% test, 10% evaluation)
X_train, X_temp, y_train, y_temp = train_test_split(X_standardized, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)

# Split the temporary set into test and evaluation sets (50% each of the 20%, so 10% of the total each)
X_test, X_eval, y_test, y_eval = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)

# List of models to evaluate
models = {
    "Random Forest": RandomForestClassifier(random_state=42),
    "Support Vector Machine (SVM)": SVC(probability=True, random_state=42),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Multi-layer Perceptron (MLP)": MLPClassifier(random_state=42, max_iter=1000),
    "Gradient Boosting Classifier": GradientBoostingClassifier(random_state=42)
}

# Train, predict, and evaluate each model
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')

    print(f"\n{name} Results:")
    print("Accuracy:", accuracy)
    print("Precision:", precision)
    print("Recall:", recall)
    print("F1 Score:", f1)
    print("Classification Report:\n", classification_report(y_test, y_pred, target_names=label_encoder.classes_))
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

    # Visualize confusion matrix
    sns.set(style="whitegrid")
    plt.figure(figsize=(8, 6))
    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt="d", cmap="Blues", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
    plt.xlabel("Predicted", fontsize=14)
    plt.ylabel("Actual", fontsize=14)
    plt.title(f"Confusion Matrix ({name})", fontsize=16)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    plt.tight_layout()
    plt.savefig(f"{name.replace(' ', '_').lower()}_confusion_matrix.png")
    plt.show()

    # ROC Curve
    plt.figure(figsize=(6, 4))
    for i in range(len(label_encoder.classes_)):
        fpr, tpr, _ = roc_curve(y_test, y_prob[:, i], pos_label=i)
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, lw=2, label=f'Class {label_encoder.classes_[i]} (area = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate', fontsize=14)
    plt.ylabel('True Positive Rate', fontsize=14)
    plt.title(f'Receiver Operating Characteristic ({name})', fontsize=16)
    plt.legend(loc="lower right", fontsize=12)
    plt.tight_layout()
    plt.savefig(f"{name.replace(' ', '_').lower()}_roc_curve.png")
    plt.show()

    # Precision-Recall Curve
    plt.figure(figsize=(6, 4))
    for i in range(len(label_encoder.classes_)):
        precision, recall, _ = precision_recall_curve(y_test, y_prob[:, i], pos_label=i)
        plt.plot(recall, precision, lw=2, label=f'Class {label_encoder.classes_[i]}')
    plt.xlabel('Recall', fontsize=14)
    plt.ylabel('Precision', fontsize=14)
    plt.title(f'Precision-Recall Curve ({name})', fontsize=16)
    plt.legend(loc="lower left", fontsize=12)
    plt.tight_layout()
    plt.savefig(f"{name.replace(' ', '_').lower()}_precision_recall_curve.png")
    plt.show()

import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Load the dataset
data = pd.read_csv("/content/wqi_data_with_wqc (1).csv")

# Separate features (X) and target (Y)
X = data.drop(columns=['WQI', 'WQC'])
y = data['WQC']

# Encode the target labels to numeric values
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Initialize scalers
min_max_scaler = MinMaxScaler()
standard_scaler = StandardScaler()

# Normalize the data
X_normalized = min_max_scaler.fit_transform(X)

# Standardize the data
X_standardized = standard_scaler.fit_transform(X_normalized)

# Split the data: 80% train, 20% temporary split (to later split into 10% test, 10% evaluation)
X_train, X_temp, y_train, y_temp = train_test_split(X_standardized, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)

# Split the temporary set into test and evaluation sets (50% each of the 20%, so 10% of the total each)
X_test, X_eval, y_test, y_eval = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)

# List of models to evaluate
models = {
    "Random Forest": RandomForestClassifier(random_state=42),
    "Support Vector Machine (SVM)": SVC(probability=True, random_state=42),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Multi-layer Perceptron (MLP)": MLPClassifier(random_state=42, max_iter=1000),
    "Gradient Boosting Classifier": GradientBoostingClassifier(random_state=42)
}

# Initialize dictionary to store results
classification_results = []

# Train, predict, and evaluate each model
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')

    classification_results.append({
        "Model": name,
        "Accuracy": accuracy,
        "Precision": precision,
        "Recall": recall,
        "F1 Score": f1
    })

    print(f"\n{name} Results:")
    print("Accuracy:", accuracy)
    print("Precision:", precision)
    print("Recall:", recall)
    print("F1 Score:", f1)
    print("Classification Report:\n", classification_report(y_test, y_pred, target_names=label_encoder.classes_))
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

    # Visualize confusion matrix
    sns.set(style="whitegrid")
    plt.figure(figsize=(8, 6))
    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt="d", cmap="Blues", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
    plt.xlabel("Predicted", fontsize=14)
    plt.ylabel("Actual", fontsize=14)
    plt.title(f"Confusion Matrix ({name})", fontsize=16)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    plt.tight_layout()
    plt.savefig(f"{name.replace(' ', '_').lower()}_confusion_matrix.png")
    plt.show()

    # ROC Curve
    plt.figure(figsize=(6, 4))
    for i in range(len(label_encoder.classes_)):
        fpr, tpr, _ = roc_curve(y_test, y_prob[:, i], pos_label=i)
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, lw=2, label=f'Class {label_encoder.classes_[i]} (area = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate', fontsize=14)
    plt.ylabel('True Positive Rate', fontsize=14)
    plt.title(f'Receiver Operating Characteristic ({name})', fontsize=16)
    plt.legend(loc="lower right", fontsize=12)
    plt.tight_layout()
    plt.savefig(f"{name.replace(' ', '_').lower()}_roc_curve.png")
    plt.show()

    # Precision-Recall Curve
    plt.figure(figsize=(6, 4))
    for i in range(len(label_encoder.classes_)):
        precision, recall, _ = precision_recall_curve(y_test, y_prob[:, i], pos_label=i)
        plt.plot(recall, precision, lw=2, label=f'Class {label_encoder.classes_[i]}')
    plt.xlabel('Recall', fontsize=14)
    plt.ylabel('Precision', fontsize=14)
    plt.title(f'Precision-Recall Curve ({name})', fontsize=16)
    plt.legend(loc="lower left", fontsize=12)
    plt.tight_layout()
    plt.savefig(f"{name.replace(' ', '_').lower()}_precision_recall_curve.png")
    plt.show()

# Save classification results to CSV
classification_results_df = pd.DataFrame(classification_results)
classification_results_df.to_csv("classification_results.csv", index=False)

